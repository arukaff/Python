{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "S8F8_NNT54Zf",
    "outputId": "fd20be43-ad53-4080-e974-69494f06aa71"
   },
   "outputs": [],
   "source": [
    "pip install pyspark >> None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "asJ5h-5q8LA2"
   },
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'pyspark'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m SparkSession\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m col, \u001b[38;5;28msum\u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mpyspark\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01msql\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mfunctions\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;241m*\u001b[39m\n",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'pyspark'"
     ]
    }
   ],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import col, sum\n",
    "from pyspark.sql.functions import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hJONvAix5zzX"
   },
   "source": [
    "### Задание 1\n",
    "Вывод данных в консоль\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "yTvqNZo758jp"
   },
   "source": [
    "1. Создаем сессию Spark с именем \"CountElements\".\n",
    "2. Читаем поток данных с использованием источника \"rate\", который автоматически генерирует данные. Этот источник используется для тестирования и отладки, поскольку он позволяет легко создавать потоки данных без необходимости подключения к внешним системам.\n",
    "3. Записываем полученные данные в консоль с использованием режима вывода \"append\". В этом режиме в консоль будут выводиться только новые строки, полученные в каждом микро-пакете данных.\n",
    "4. Запускаем поток обработки данных и ожидает его завершения.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "5K_V_6Kf3-nT"
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "\n",
    "spark = SparkSession.builder.appName(\"CountElements\").getOrCreate()\n",
    "df = spark.readStream.format(\"rate\").load()\n",
    "query = df.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "X6yBx_7h7GeD"
   },
   "source": [
    "### Задание 2:\n",
    "Фильтрация чисел (четные)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3q_5imWc4i7h"
   },
   "source": [
    "1. Создаем сессию Spark с именем приложения \"FilterEvenNumbers\". Это необходимо для инициализации Spark и подготовки к выполнению операций с данными.\n",
    "\n",
    "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и демонстрации обработки потоковых данных.\n",
    "\n",
    "3. Фильтруем полученные данные, оставляя только те строки, где значение поля \"value\" делится на 2 без остатка, то есть числа, являющиеся четными.\n",
    "\n",
    "4. Записываем отфильтрованные данные обратно в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоке, новые данные будут добавлены к уже существующим, не удаляя предыдущие записи.\n",
    "\n",
    "5. Запускаем потоковую запись и ожидает завершения работы запроса в течение 5 секунд. Это позволяет увидеть результаты обработки данных в консоли в течение этого времени.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HrT_u-Te6O_u",
    "outputId": "8069bc9d-cded-47bc-a006-929f87846386"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"FilterEvenNumbers\").getOrCreate()\n",
    "df = spark.readStream.format(\"rate\").load()\n",
    "df_even = df.filter(\"value % 2 == 0\")\n",
    "query = df_even.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YtgLHHmE7f9D"
   },
   "source": [
    "### Задание 3:\n",
    "Сгруппировать по значениям и посчитать количество\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "zCsmYvdT4vzL"
   },
   "source": [
    "1. Создаем сессию Spark с именем приложения \"GroupByValue\". Это необходимо для работы с Spark и его функциональностью, включая потоковую обработку данных.\n",
    "\n",
    "2. Читаем потоковые данные из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с постоянной скоростью, что удобно для тестирования и демонстрации потоковой обработки.\n",
    "\n",
    "3. Группируем полученные данные по столбцу \"value\" и подсчитывает количество записей для каждого уникального значения в этом столбце. Это позволяет агрегировать данные по определенному критерию.\n",
    "\n",
    "4. Запускаем потоковую запись результатов агрегации в консоль с использованием режима вывода \"update\". Режим \"update\" означает, что в консоль будут выводиться только обновленные результаты агрегации, а не полный набор данных при каждом обновлении. Это делает вывод более читаемым и эффективным, особенно когда данные постоянно обновляются.\n",
    "\n",
    "5. Ожидаем завершения потоковой обработки в течение 5 секунд. Это означает, что поток будет работать в течение этого времени, а затем завершится.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "CNG16a4I7LFA",
    "outputId": "588fdabd-18a8-4d9f-e462-249a6da8e2a1"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"GroupByValue\").getOrCreate()\n",
    "df = spark.readStream.format(\"rate\").load()\n",
    "df_grouped = df.groupBy(\"value\").count()\n",
    "query = df_grouped.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "query.awaitTermination(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "L-mPJYiE7mKI"
   },
   "source": [
    "### Задание 4\n",
    "Вычислить сумму значений\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fBpp4yFv48Qt"
   },
   "source": [
    "1. Создаем экземпляр SparkSession с именем приложения \"SumValues\". Этот шаг необходим для инициализации Spark и подготовки среды для выполнения операций с данными.\n",
    "\n",
    "2. Читаем поток данных из источника, используя формат \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки. В данном случае, данные будут генерироваться автоматически без необходимости подключения к внешним источникам данных.\n",
    "\n",
    "3. Выполняем агрегацию данных, вычисляя сумму значений в столбце \"value\" и сохраняя результат в новом столбце \"total\". Это делается с помощью метода selectExpr, который позволяет выполнять SQL-подобные выражения для трансформации данных.\n",
    "\n",
    "4. Записываем результаты агрегации в консоль с использованием формата \"console\". Это позволяет наблюдать за изменениями в реальном времени, так как данные будут выводиться в консоль.\n",
    "\n",
    "5. Запускаем запрос на потоковую обработку данных с использованием режима вывода \"update\". В этом режиме, при каждом обновлении данных, будет выводиться только текущее состояние агрегации, что позволяет видеть актуальную сумму значений.\n",
    "\n",
    "6. Ожидаем завершения обработки потока данных в течение 5 секунд с помощью метода awaitTermination. Это означает, что приложение будет работать в течение этого времени, обрабатывая поступающие данные и выводя результаты в консоль.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 211
    },
    "id": "xiJZWI6f7lzl",
    "outputId": "b13d34ef-8c34-4a6e-a242-77611147035a"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'SparkSession' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-1f6543506e4a>\u001b[0m in \u001b[0;36m<cell line: 1>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mspark\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mSparkSession\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbuilder\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappName\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"SumValues\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgetOrCreate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      2\u001b[0m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspark\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"rate\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mdf_sum\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mselectExpr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"sum(value) AS total\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mquery\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf_sum\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwriteStream\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputMode\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"update\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mformat\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"console\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mstart\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0mquery\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mawaitTermination\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'SparkSession' is not defined"
     ]
    }
   ],
   "source": [
    "spark = SparkSession.builder.appName(\"SumValues\").getOrCreate()\n",
    "df = spark.readStream.format(\"rate\").load()\n",
    "df_sum = df.selectExpr(\"sum(value) AS total\")\n",
    "query = df_sum.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "query.awaitTermination(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "vSk3qT1I7vCi"
   },
   "source": [
    "### Задание 5\n",
    "Найти максимальное значение"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "U4YRbmkS5J4X"
   },
   "source": [
    "1. Создаем сессию Spark с именем приложения \"MaxValue\" с помощью SparkSession.builder.appName(\"MaxValue\").getOrCreate(). Это необходимо для работы с Spark и его функциональностью, включая структурированный потоковый анализ.\n",
    "\n",
    "2. Читаем поток данных с использованием источника \"rate\". Источник \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и разработки. В данном случае, данные читаются без дополнительных параметров, что означает использование значений по умолчанию для генерации данных.\n",
    "\n",
    "3. Выполняем агрегацию данных, вычисляя максимальное значение поля 'value' в каждом микро-батче данных. Это достигается с помощью метода agg(f.max('value')), где f - это ссылка на модуль pyspark.sql.functions, который предоставляет функции для работы с данными.\n",
    "\n",
    "4. Записываем результаты агрегации в консоль с использованием writeStream.outputMode(\"update\").format(\"console\").start(). Здесь outputMode(\"update\") указывает, что при каждом обновлении данных в потоке, результаты агрегации будут выводиться в консоль. format(\"console\") указывает, что вывод должен быть направлен в консоль, а не в файл или базу данных.\n",
    "\n",
    "5. Ожидаем завершения потокового запроса с помощью query.awaitTermination(5), что означает, что поток будет работать в течение 5 секунд, после чего будет выполнено завершение работы.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "DjPrxvmF5QKc"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"MaxValue\").getOrCreate()\n",
    "df = spark.readStream.format(\"rate\").load()\n",
    "df_max = df.agg(f.max('value'))\n",
    "query = df_max.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "query.awaitTermination(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "nkgaFpi472pT"
   },
   "source": [
    "### Задание 6\n",
    "Вычислить скользящее окно по значению\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "G2wCQyBC5aD6"
   },
   "source": [
    "1. Создаем сессию Spark с именем \"SlidingWindow\".\n",
    "2. Читаем потоковые данные из источника \"rate\", который генерирует данные с постоянной скоростью.\n",
    "3. Группируем эти данные по временным окнам, размером в 10 минут, и подсчитывает количество записей в каждом окне.\n",
    "4. Записываем результаты обработки в консоль в режиме обновления (outputMode(\"update\")), что означает, что в консоль будут выводиться только обновленные агрегированные результаты.\n",
    "5. Запускаем потоковую запрос и ожидает его завершения в течение 5 секунд.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgGCMPhD5Rnp"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"SlidingWindow\").getOrCreate()\n",
    "df = spark.readStream.format(\"rate\").load()\n",
    "df_windowed = df.groupBy(window(\"timestamp\", \"10 minutes\")).count()\n",
    "query = df_windowed.writeStream.outputMode(\"update\").format(\"console\").start()\n",
    "query.awaitTermination(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F9Tm_dbj79n8"
   },
   "source": [
    "### Задание 7\n",
    "Соединение потоков данных"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "013d5A_d5luj"
   },
   "source": [
    "1. Создаем сессию Spark с именем приложения \"JoinStreams\".\n",
    "2. Читаем два потока данных, используя формат \"rate\", который генерирует данные с определенной скоростью. По умолчанию, каждый поток будет генерировать одну строку в секунду с полями timestamp и value.\n",
    "3. Соединяем два потока данных по полю value. Это означает, что для каждой пары строк из двух потоков, где значение поля value совпадает, будет создана новая строка в результате соединения.\n",
    "4. Записываем результат соединения в консоль с использованием режима вывода \"append\". Это означает, что при каждом обновлении данных в потоках, новые строки будут добавляться к выводу, не удаляя предыдущие строки.\n",
    "5. Запускаем запрос на потоковую обработку и ожидает его завершения в течение 5 секунд.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EhpFy8Ut76yx"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"JoinStreams\").getOrCreate()\n",
    "df1 = spark.readStream.format(\"rate\").load()\n",
    "df2 = spark.readStream.format(\"rate\").load()\n",
    "df_joined = df1.join(df2, \"value\")\n",
    "query = df_joined.writeStream.outputMode(\"append\").format(\"console\").start()\n",
    "query.awaitTermination(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wzea5l1S8C9I"
   },
   "source": [
    "### Задание 8\n",
    "Запись данных в файл\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "me1-Pd1e5vE_"
   },
   "source": [
    "1. Создаем экземпляр SparkSession с именем приложения \"WriteToFile\". SparkSession является точкой входа для работы с Spark и предоставляет API для работы с DataFrame и DataSet.\n",
    "\n",
    "2. Читаем потоковые данные с использованием формата \"rate\". Формат \"rate\" генерирует данные с заданной скоростью, что полезно для тестирования и отладки потоковых приложений. В данном случае, данные будут генерироваться без дополнительных параметров, что означает использование значений по умолчанию.\n",
    "\n",
    "3. Записываем полученные потоковые данные в формате Parquet в указанный каталог \"output/\". Parquet - это эффективный формат хранения данных, который обеспечивает высокую производительность и эффективное сжатие.\n",
    "\n",
    "4. Запускаем потоковую запись данных и ожидает завершения работы запроса в течение 5 секунд. Метод awaitTermination блокирует выполнение программы до тех пор, пока потоковая запись не будет остановлена или не произойдет ошибка.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "EqhFZzIB8DvS"
   },
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName(\"WriteToFile\").getOrCreate()\n",
    "df = spark.readStream.format(\"rate\").load()\n",
    "query = df.writeStream.format(\"parquet\").option(\"path\", \"output/\").start()\n",
    "query.awaitTermination(5)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
